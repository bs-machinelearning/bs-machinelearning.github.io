<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <title>BSML | Bocconi Students for Machine Learning</title>
    <meta content="" name="description">
    <meta content="" name="keywords">
  
    <!-- Favicons -->
    <link href="../assets/img/white_bg.png" rel="icon">
    <link href="../assets/img/white_bg.png" rel="apple-touch-icon">
  
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com" rel="preconnect">
    <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap" rel="stylesheet">
  
    <!-- Boostrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.3.0/font/bootstrap-icons.css">
  
    <!-- AOS -->
    <link href="https://unpkg.com/aos@2.3.1/dist/aos.css" rel="stylesheet">
  
    <!-- Main CSS -->
    <link href="../assets/css/main.css" rel="stylesheet">
  
    <!-- =======================================================
    * Template Name: HeroBiz
    * Template URL: https://bootstrapmade.com/herobiz-bootstrap-business-template/
    * Updated: Jun 29 2024 with Bootstrap v5.3.3
    * Author: BootstrapMade.com
    * License: https://bootstrapmade.com/license/
    ======================================================== -->
</head>

<body class="blog-details-page">

    <header id="header" class="header d-flex align-items-center sticky-top">
        <div class="container-fluid position-relative d-flex align-items-center justify-content-between">
    
          <a href="index.html" class="logo d-flex align-items-center me-auto me-xl-0">
            <!-- Uncomment the line below if you also wish to use an image logo -->
            <!-- <img src="assets/img/logo.png" alt=""> -->
            <h1 class="sitename">BSML</h1>
          </a>
    
          <nav id="navmenu" class="navmenu">
            <ul>
              <li><a href="../index.html#home">Home<br></a></li>
              <li><a href="../index.html#about">About</a></li>
    <!--           <li><a href="#pricing">Membership</a></li> -->
              <li><a href="../index.html#faq">FAQ</a></li>
              <li><a href="../index.html#team">Founders</a></li>
              <li><a href="../projects.html" class="active">Projects</a></li>
              <li><a href="../index.html#footer">Contact</a></li>
            </ul>
            <i class="mobile-nav-toggle d-xl-none bi bi-list"></i>
          </nav>
    
          <a class="btn-getstarted" href="index.html#open-roles">JOIN US</a>
    
        </div>
    </header>

    <main class="main">

        <!-- Page Title -->
        <!-- <div class="page-title">
        <div class="container d-lg-flex justify-content-between align-items-center">
            <h1 class="mb-2 mb-lg-0">Blog Details</h1>
            <nav class="breadcrumbs">
            <ol>
                <li><a href="index.html">Home</a></li>
                <li class="current">Blog Details</li>
            </ol>
            </nav>
        </div>
        </div> -->
        <!-- End Page Title -->

        <div class="container">
        <div class="row">

            <div class="col-lg-8">

                <!-- Blog Details Section -->
                <section id="blog-details" class="blog-details section">
                    <div class="container">

                    <article class="article">

                        <div class="post-img">
                        <img src="../assets/img/proj/proj-1.jpg" alt="" class="img-fluid">
                        </div>
                        <h2 class="title">Digital Symbiosis: How Machines Learn from the Human Mind</h2>

                        <div class="meta-top">
                        <ul>
                            <li class="d-flex align-items-center"><i class="bi bi-person"></i> <a href="blog-details.html">Marco Beltramin</a></li>
                            <li class="d-flex align-items-center"><i class="bi bi-clock"></i> <a href="blog-details.html"><time datetime="2020-01-01">Jan 19, 2023</time></a></li>
                        </ul>
                        </div><!-- End meta top -->

                        <div class="content">
                        <p>
                            In the wide field of Machine Learning there is one common problem that afflicts even the most cutting-edge models: alignment.
                            This term is used to describe the process by which researchers ensure that machines do not only complete the task they are assigned, 
                            but also do it in a way that is fully aligned with the human intentions and set of values.
                        </p>
                        <p>
                            How can you explain to the car the meaning of safety or caution? And most importantly how can we train machines to weigh the potential risks or 
                            dangers of a situation in a manner similar to the human brain?
                        </p>

                        <blockquote>
                            <p>
                                A deep dive into the alignment problem and the newest techniques to mitigate it.
                            </p>
                        </blockquote>
                        <p>
                            It is safe to say that traditional models used in ML are not adequate to complete this task at a high level. 
                            This challenge is evident even in the most used AI tool worldwide: ChatGPT.
                        </p>
                        <p>
                            Technically speaking, ChatGPT is a large language model, or LLM, that generates human-like responses by probabilistically predicting tokens, 
                            which are pieces of words, in a sequence. Starting from a human prompt, the model selects the next word in the response based on the probabilities 
                            it learned during its training process, which involved analysing vast datasets of human language. In this scenario, the machine learns from 
                            humans through a part of the algorithm known as the loss function. Consider the loss function as an evaluation system used to assess the 
                            performance of the model. If ChatGPT is given the task of producing an accurate response, the function checks how closely the machine's answer 
                            matches the human one and assigns a score. Subsequently, much like a diligent student, the machine strives to achieve the best score possible. By 
                            doing so, it updates its own weights (the parameters that generate the response) to produce an answer that is as closely aligned with the training 
                            data as possible.
                        </p>
                        <p>
                            The problem with this approach is that the loss function equally penalizes a wide range of errors which are, from a human prospective, everything 
                            but equal. In producing a sentence some errors can be more problematic than others, for example misspelling a word can be less problematic than 
                            using a wrong word that completely change the meaning of the sentence. This bias is particularly evident in context where you need a high degree 
                            of subjectiveness or creativity, and correctness is really difficult to measure in an analytic way such as poetry or philosophy.
                        </p>
                        <p>
                            This problem doesn't only deal with correctness in a strict sense, but also with the overall quality of the response. Have you ever received 
                            a 300-word response from Chat GPT and thought, 'Wow, it really didn't give me any new information'? Well, if so, you probably have an 
                            alignment issue too! Remember that ChatGPT and LLM in general are built to give you the right answer grammatically and syntactically speaking, 
                            not necessarily the most suited one to your prompt. So sometimes this incredibly “intelligent” machines tend to avoid putting themselves in 
                            a bad position when producing an answer and just opt to respond in the easiest way possible that minimize their loss function. This type of 
                            reward-hacking is even more evident when the LLM produces hallucinations - i.e., false statements that sounds convincing but are completely 
                            false or nor verifiable. In this situation, the problem arises from the fact that the loss function may prioritize the fluidity of the speech 
                            and its tone more than the overall veridicity, or also from the problem that a lot of LLM are not connected to third party fact checker APIs 
                            that give the model true or false signal when producing an answer (during the so-called inference phase).
                        </p>
                        <h3>RLHF: Bridging the Alignment Gap</h3>
                        <p>
                            Starting from 2020, researchers from OpenAI have introduced a new technique to mitigate this problem: Reinforcement Learning from Human Feedback or RLHF.
                        </p>
                        <p>
                            As suggested by the name, this technique is based on the principles of reinforcement learning but, instead of having the agent to explore 
                            the environment and produce decisions based on multiple attempts and direct reward signals, it has to rely on an historical register of 
                            “good actions” or in this case good responses ranked by human annotators.
                        </p>
                        <p>
                            Recently, a meme that grotesquely represents the concept of RLHF has gone viral and, while being a little bit exaggerated, it illustrates 
                            pretty clearly the main concept of it. Imagine the training data of a LLM as a vast and wild amount of data taken from every part of the 
                            web: academic papers, newspaper articles, but also posts on social network and blogs where users interact with a very different range of 
                            words and styles. This unstructured and chaotic pool of data is subsequently refined, with a significant amount of irrelevant or extraneous 
                            content removed. As a result, the model begins to behave and respond in a manner akin to a human being. And finally the “cherry on top“: RLHF 
                            enables the model to eliminate a lot of toxicity, hallucinations and to be more aligned to the users request.
                        </p>
                        <img src="../assets/img/proj/digital-symbiosis-1.png" class="img-fluid" alt="">

                        <p>
                            To explain in more technical terms, the process begins with the LLM producing responses, which are then evaluated and ranked by human 
                            annotators according to criteria such as truthfulness, usefulness, and safety.  Based on these responses, a reward model is then trained 
                            with the goal of giving a score to the next responses generated from the original LM model. The greater the similarity between the new 
                            answer and the best answer as judged by humans, the better the score received by the reward model.
                        </p>
                        <img src="../assets/img/proj/digital-symbiosis-2.png" class="img-fluid" alt="">
                        <p>
                            It' s obvious that in this case the second response is going to be preferred by a human annotator. While the first response is less 
                            helpful and more offensive, it still follows a logical pattern, it's grammatically correct and gives a (terrible) answer that covers all 
                            the topic also mentioned by answer number 2. In this case the human judgment is necessary for understanding the nuances of the concept of 
                            correctness and train the reward model accordingly.
                        </p>
                        <p>
                            With this logic, the reward model is going to be trained on a lot of this type of preferences and it will “learn” and recompense the 
                            responses that match the human choice of word, syntax and tone.
                        </p>
                        <p>
                            Based on these rewards, the model is going to generate more answers with the goal of having them score an higher grade on the reward 
                            model scale and if they don’t, the model is going to update its text generation parameters to modify the probabilistic generation of 
                            test until it reaches a threshold score.
                        </p>
                        <p>
                            Unfortunately, LLMs are not that easy to train and, on the contrary, they are very good in finding loopholes to maximize the reward. 
                            Let's look at what an incorrect response the LLM can give based on the reward model metrics and the preferred second response:
                        </p>
                        <img src="../assets/img/proj/digital-symbiosis-3.png" class="img-fluid" alt="">
                        <p>
                            Maybe, in this case, the reward model has taken the assignment of replicating human preference a little bit too serious, so much that 
                            this answer looks everything but human. The reward model is very likely to give this answer a high score because it recalls very 
                            closely answer number two: it has a similar syntax with bullet points, it emphasizes the helpfulness of Bad Bunny's album to the 
                            humanity, it uses positive words that are very far from an offensive context and uses a very technical approach based on ML in evaluating 
                            the performance of the album.
                        </p>
                        <p>
                            But is this response aligned to the user values and preferences? Clearly not.
                        </p>
                        <p>
                            Researchers have discovered that the process needs a further step between the reward model training and the parameters update. They 
                            want to make sure that the token probability distribution of the updated model does not diverge very much from the initial one giving 
                            penalties to this type of answers based on a KL divergence metric, a measure of how much one probability distribution differs from a 
                            reference distribution. Basically, the goal is not to completely overhaul the way the model responds to answers based solely on what the 
                            reward model has recently seen. In our case, they don't want a simple question to be answered using a ML approach, inappropriate positive 
                            words, and a format based on bullet points - ChatGPT really likes this one, doesn't it?
                        </p>
                        <h3>Balancing The Scale: The Strength and Weaknesses of RLHF</h3>
                        <p>
                            As we have seen, the RLHF isn't a straightforward method and has a lot of complications and the need of, often expensive, human works. Is it worth it?
                        </p>
                        <p>
                            A recent study from OpenAI compared its model instructed with RLHF “InstructGPT” with the base GPT3 model and the results are the following:
                        </p>
                        <img src="../assets/img/proj/digital-symbiosis-4.png" class="img-fluid" alt="">
                        <p>
                            From the results, it's clear the RLHF has proved superior in some tasks, especially in avoiding creating toxic, wrong and context-inappropriate 
                            content thanks to the knowledge learned from human annotators, but is this process efficient and sustainable?
                        </p>
                        <p>
                            The answer unfortunately is probably negative because RLHF has multiple bottlenecks that makes it a very costly and long process. Let's examine the reasons:
                            <ol>
                                <li>
                                    THE HUMAN FACTOR: learning by humans can be a great way for a LLM, but it comes with a lot of problems. Firstly, it is necessary 
                                    that all the annotators for the reward model are from different ethnicity, cultural backgrounds, age and religion in order to avoid that 
                                    the model only replicates the behaviour of a subset of the population. Secondly, it can occur that humans fail to completely understand 
                                    the given tasks and thus correctly classify the answer.
                                </li>
                                <li>
                                    OVERFITTING: there is a huge risk that the model is going to overfit if the data provided to the reward model are too few or too specific. 
                                    It is also important to accurately calibrate the algorithm, named PPO or Proximal Policy Optimization, that prevents the model from changing 
                                    too much the probability distribution of word creation.
                                </li>
                                <li>
                                    COMPLEXITY OF REWARD MODEL: designing a model that captures a whole range of human preferences is undoubtedly difficult. As of now, 
                                    the preferred loss function used is a “pairwise ranking loss” that just measure in a very general and comprehensive way how much the answer 
                                    differs from the preferred one, without actually taking into consideration in which ways it differs from it. For instance, looking at 
                                    example N2 it will be amazing to tell the model that in this case the response very much differs from a human one because it's too formal, 
                                    too technical and syntactically out of context; in this way it could start to learn how many different faces the human preferences have 
                                    and to use this information for better content creation.
                                </li>
                            </ol>
                        </p>
                        <p>So, What's Next?</p>
                        <p>
                            Lately, there has been a lot of interest and research on RLHF, even ChatGPT has started asking users which response they prefer from a set of two. 
                            ML practitioners have focused their efforts on eliminating the bottleneck that deals with the fact that human feedback is often non scalable and 
                            inefficient to collect.
                        </p>
                        <p>
                            The most promising study comes from NVIDIA, with their new approach called “SteerLM”. This method apparently allows to enrich the model with an 
                            augmented database generated by AI, but still fundamentally based on human feedback, and to control the model settings at inference phase in order 
                            to dynamically change the style of the response based on user's preferences.
                        </p>
                        <p>
                            This approach is based on the training of an Attribution Prediction Model or APM, based on a dataset where human annotators once again have 
                            expressed their preferences. The main difference here is that the humans have not only ranked them from worst to best in an overall way, but 
                            they have classified their performance in terms of a various set of attributes such as helpfulness, humour, and creativity. The model can evaluate 
                            multiple nuances of a single output and try to expand its training data by evaluating pre-made databases (used for various purposes in the ML world) 
                            with the APM. Now the model has successfully expanded its dataset, bypassing the non-scalability of human feedback. The new information, while not being 
                            specifically evaluated by annotators, is still a reflex of their previous work.
                        </p>
                        <p>
                            Now the machine can complete its training with the usual phase of supervised fine tuning, that is confronting a newly generated output with a new 
                            and smaller labelled dataset, in order to verify the performance of the model. Also, in this procedure, the Nvidia researchers have decided to add 
                            a twist: instead of just training the model to generate correct responses, the model is trained to generate responses that align with certain attributes 
                            (for example a response that is both high-quality and helpful) in order to be more flexible and aligned to users' requests.
                        </p>
                        <p>
                            If this wasn't enough the model is also capable of being more user-steerable, namely being able to ask to the user, before producing the output, 
                            what characteristics the response should have. Imagine this new technique applied in the gaming industry where non playable characters would not be 
                            static anymore, but capable to react to the users' actions and preferences. Furthermore, this new approach could lead to a new paradigm in the LLM 
                            industry where models are not a pre-made black box, but a new instrument capable to align with users.
                        </p>
                        <p>
                            As of now, the model has not been integrated to the suit of Nvidia's AI tools available to game developers so there is still not much buzz around it 
                            and it's not possible to find reviews. Only time will tell how this new approach to RLHF will play out.
                        </p>
                        <p>
                            The only certain thing is that hundreds of researchers are working on ways to better align AI models and the most important bottleneck is the 
                            need of expensive and often unprecise human work. Will this result in a future where our preferences and our intrinsic knowledge of abstract 
                            concepts will be replicated by a complex and evolved attribution prediction model? And at this point, what will be our role in the training of 
                            artificial intelligences if these can even replicate the most subtle facets of human beings? The answer may redefine our relationship with technology, 
                            as we pivot from direct trainers to spectators of intelligent systems that can learn, adapt, and perhaps even understand, with a finesse that rivals our own.
                        </p>
                        <!-- <img src="../assets/img/proj/digital-symbiosis-5.png" class="img-fluid" alt=""> -->
                        </div><!-- End post content -->

                        <div class="meta-bottom">
                        <!-- <i class="bi bi-folder"></i>
                        <ul class="cats">
                            <li><a href="#">Business</a></li>
                        </ul>

                        <i class="bi bi-tags"></i>
                        <ul class="tags">
                            <li><a href="#">Creative</a></li>
                            <li><a href="#">Tips</a></li>
                            <li><a href="#">Marketing</a></li>
                        </ul> -->
                        </div>
                        <!-- End meta bottom -->

                    </article>

                    </div>
                </section><!-- /Blog Details Section -->

                <!-- Blog Author Section -->
                <section id="blog-author" class="blog-author section">

                    <div class="container">
                    <div class="author-container d-flex align-items-center">
                        <img src="../assets/img/team/marco-beltramin.png" class="rounded-circle flex-shrink-0" alt="">
                        <div>
                        <h4>Marco Beltramin</h4>
                        <div class="social-links">
                            <a href="https://x.com/#"><i class="bi bi-twitter-x"></i></a>
                            <a href="https://facebook.com/#"><i class="bi bi-facebook"></i></a>
                            <a href="https://instagram.com/#"><i class="biu bi-instagram"></i></a>
                        </div>
                        <p>
                            MSc student in Finance at Bocconi University.
                        </p>
                        </div>
                    </div>
                    </div>

                </section><!-- /Blog Author Section -->

            </div>

            <div class="col-lg-4 sidebar">

            <div class="widgets-container">


                <!-- Recent Posts Widget -->
                <div class="recent-posts-widget widget-item">

                <h3 class="widget-title">Recent Posts</h3>

                <div class="post-item">
                    <img src="" alt="" class="flex-shrink-0">
                    <div>
                    <h4><a href="blog-details.html">n.a.</a></h4>
                    <time datetime="2020-01-01">n.a.</time>
                    </div>
                </div>
                <!-- End recent post item-->

                </div><!--/Recent Posts Widget -->

                <!-- Tags Widget -->
                <div class="tags-widget widget-item">

                <h3 class="widget-title">Tags</h3>
                <ul>
                    <li><a href="#">Natural Language Processing</a></li>
                    <li><a href="#">ChatGPT</a></li>
                    <li><a href="#">Alignment</a></li>
                    <li><a href="#">Reinforcement Learning</a></li>
                    <li><a href="#">Large Language Models</a></li>
                    <li><a href="#">RLHF</a></li>
                    <li><a href="#">Ethics</a></li>
                </ul>

                </div><!--/Tags Widget -->

            </div>

            </div>

        </div>
        </div>

    </main>

    <footer id="footer" class="footer dark-background">

    <div class="footer-top">
        <div class="container">
        <div class="row gy-4">
            <div class="col-lg-8 col-md-3 footer-links">
            <a href="index.html" class="logo d-flex align-items-center">
                <h3 class="sitename">BSML | Bocconi Students for Machine Learning</h3>
            </a>
            <div class="footer-contact pt-3">
                <p>Bocconi University</p>
                <p>Milan, Italy</p>
                <!-- <p class="mt-3"><strong>Phone:</strong> <span>+1 5589 55488 55</span></p> -->
                <p><strong>Email:</strong> <span>bsmachinelearning@gmail.com</span></p>
            </div>
            <!-- <div>
                © Copyright <strong><span>BSML</span></strong> 2024. All Rights Reserved
            </div> -->
            </div>
            <div class="col-lg-4 col-md-3 footer-links d-flex flex-row justify-content-center align-items-center">
            <div class="social-links order-first order-lg-last mb-3 mb-lg-0">
                <!-- <a href=""><i class="bi bi-twitter-x"></i></a>
                <a href=""><i class="bi bi-facebook"></i></a> -->
                <a href="https://www.instagram.com/bs.machinelearning/" target="_blank"><i class="bi bi-instagram"></i></a>
                <a href="https://it.linkedin.com/company/bocconi-students-for-machine-learning" target="_blank"><i class="bi bi-linkedin"></i></a>
                <a href="https://github.com/bs-machinelearning"target="_blank"><i class="bi bi-github"></i></a>
            </div>
            </div>
        </div>
        </div>
    </div>

    </footer>

    <!-- Scroll Top -->
    <a href="#" id="scroll-top" class="scroll-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

    <!-- Preloader -->
    <div id="preloader"></div>

    <!-- Boostrap -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>

    <!-- AOS -->
    <script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>

    <!-- Main JS File -->
    <script src="../assets/js/main.js"></script>

</body>


</html>